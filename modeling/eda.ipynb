{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exploratory data analysis file for current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/m72-ea4p-pnqp.csv\",header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"Tracking Number\"].notna().sum())\n",
    "print(data[\"Tracking Number\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a mask for \n",
    "data[data[\"Government Agency\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"Attachment Files\"].notna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations looking at the above data:\n",
    "1) Lots of columns with absolutely no data\n",
    "2) Different scenarios for records:\n",
    "    i) Records for document name (given away by records that have Federal Register number)\n",
    "    ii) Normal people who comment\n",
    "    iii) govt. agencies that comment (giveaway is non-null govt agency and govt agency type)\n",
    "    iv) file attachments - any commentor can have attachments, usually document with comments/letter\n",
    "Approach:\n",
    "1) first need to clean data so that each row corresponds to 1 comment:\n",
    "    - take document info row, remove, and place its details in each respective comment row\n",
    "    - clean data - convert date columns to pd.datetime\n",
    "    - use SBERT, BERTopic, sentence transformers primarily for text embedding\n",
    "    - store embeddings in ChromaDB\n",
    "    - identify theme/content for document\n",
    "    - identify sentiment of comment on given document\n",
    "    - PHATE should cluster (in a branch) together similar sentiments for a given topic\n",
    "    - hence should appear as numerous tree structures in viz with branches determining sentiments\n",
    "    - as you move further down branch, should indicate passing of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posted Date & Received Date need to be put into datetime format\n",
    "data[\"Posted Date\"] = pd.to_datetime(data[\"Posted Date\"],errors=\"coerce\")\n",
    "data[\"Received Date\"] = pd.to_datetime(data[\"Received Date\"],errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data[data[\"Federal Register Number\"].notna()]\n",
    "comments = data[data[\"Federal Register Number\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"is_govt_agency\"] = data[\"Government Agency\"].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding models:\n",
    "#SBERT, BERTopic, LLama, Huggingface R1, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "topic_embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# comment_perspectives = pipeline(task=\"text2text-generation\", model=\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective_model = SentenceTransformer(\"nli-roberta-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data = comments[\"Comment\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"topic_embeddings.npy\" in files:\n",
    "    print(\"True!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"topic_embeddings.npy\" in files:\n",
    "    topic_embeddings = np.load(\"topic_embeddings.npy\")\n",
    "else:\n",
    "    topic_embeddings = topic_embedder.encode(comment_data,show_progress_bar=True)\n",
    "    np.save(\"topic_embeddings.npy\", topic_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"perspective_embeddings.npy\" in files:\n",
    "    perspective_embeddings = np.load(\"perspective_embeddings.npy\")\n",
    "else:\n",
    "    perspective_embeddings = perspective_model.encode(comment_data,show_progress_bar=True)\n",
    "    np.save(\"perspective_embeddings.npy\", perspective_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate topic + perspective embeddings\n",
    "final_embeddings = np.hstack((topic_embeddings, perspective_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example timestamps (YYYY-MM-DD format)\n",
    "timestamps = pd.to_datetime(comments[\"Posted Date\"]).astype(int) / 10**9  # Convert to Unix timestamps\n",
    "timestamps = timestamps.to_numpy()\n",
    "\n",
    "# Normalize timestamps to [0,1] range\n",
    "scaler = MinMaxScaler()\n",
    "normalized_timestamps = scaler.fit_transform(timestamps.reshape(-1, 1))\n",
    "\n",
    "# Append normalized time to embeddings\n",
    "final_embeddings = np.hstack((final_embeddings, normalized_timestamps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(topic_embeddings)\n",
    "\n",
    "# Apply HDBSCAN to cluster comments into topics\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, metric=\"euclidean\", cluster_selection_method=\"eom\")\n",
    "comments[\"topic_cluster\"] = clusterer.fit_predict(scaled_embeddings)\n",
    "\n",
    "# Count the number of topics\n",
    "num_topics = len(set(comments[\"topic_cluster\"])) - (1 if -1 in comments[\"topic_cluster\"].values else 0)\n",
    "print(f\"Number of Topics Identified: {num_topics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run PHATE\n",
    "phate_operator = phate.PHATE()\n",
    "phate_embedding = phate_operator.fit_transform(final_embeddings)\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(phate_embedding[:, 0], phate_embedding[:, 1], c=normalized_timestamps, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Time Progression\")\n",
    "plt.title(\"Comment Evolution Over Time\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
