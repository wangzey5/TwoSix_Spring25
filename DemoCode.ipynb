{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00725cf2-9a30-4780-af28-d1cc368a847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.regulationsgov_test # Testing DB\n",
    "comments_collection = db.comments\n",
    "details_collection = db.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850418c8-6c0f-451e-93fb-e836725b61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import time\n",
    "from data_collection.CommentScraper import exists, insert\n",
    "\n",
    "## Custom getAllComments breaks after 2 pages\n",
    "def getAllComments(apibasereq, collection):\n",
    "    pageNum = 1\n",
    "    metaPageNum = 1\n",
    "    while True: \n",
    "        apireq = deepcopy(apibasereq)\n",
    "        try:\n",
    "            documents = apireq.sort(\"lastModifiedDate\").page(pageNum).get()\n",
    "            print(f\"[{metaPageNum}](pg {pageNum}/20) ratelimit={apireq.ratelimit}\", end=\"\")\n",
    "            print(\" \"*100, end=\"\\r\")\n",
    "        except RuntimeError:\n",
    "            print(\"Rate Limit exceeded, retrying in 1 minute\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        except ConnectionError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        if len(documents[\"data\"]) == 0:\n",
    "            break\n",
    "\n",
    "        for doc in documents[\"data\"]:\n",
    "            if exists(doc, collection):\n",
    "                continue\n",
    "            insert(doc, collection)\n",
    "\n",
    "        if pageNum >= 2: ## TESTING ONLY\n",
    "            break        #\n",
    "        \n",
    "        if documents[\"meta\"][\"hasNextPage\"] == False:\n",
    "            date = documents[\"data\"][-1][\"attributes\"][\"lastModifiedDate\"]\n",
    "            apireq = apireq.lastmodified(date)\n",
    "            pageNum = 1\n",
    "            metaPageNum += 1\n",
    "        else:\n",
    "            pageNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dec6fd-6194-437d-8626-34e17d05228b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse filetype 'doc' for attachment https://downloads.regulations.gov/NARA-05-0005-0002/attachment_2.doc            \n",
      "The command `antiword /tmp/tmpvvq0nnq8` failed with exit code 1\n",
      "------------- stdout -------------\n",
      "b''------------- stderr -------------\n",
      "b'/tmp/tmpvvq0nnq8 is not a Word Document. It is probably a Word Perfect file\\n'\n",
      "Failed to parse filetype 'doc' for attachment https://downloads.regulations.gov/EPA-HQ-OA-2003-0006-0003/attachment_1.doc\n",
      "The command `antiword /tmp/tmpmuot01u_` failed with exit code 1\n",
      "------------- stdout -------------\n",
      "b''------------- stderr -------------\n",
      "b'/tmp/tmpmuot01u_ is not a Word Document. It is probably a Rich Text Format file\\n/tmp/tmpmuot01u_ is not a Word Document.\\n'\n"
     ]
    }
   ],
   "source": [
    "from data_collection.RegAPI import RegAPI\n",
    "from data_collection.CommentScraper import getCommentDetails, APICommentDetailScraper\n",
    "\n",
    "api = RegAPI(250)\n",
    "getAllComments(api.endpoint(\"/comments\"), comments_collection)\n",
    "comments = [comment for comment in comments_collection.find()]\n",
    "getCommentDetails(APICommentDetailScraper(api), comments, details_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f3100f-caf6-4779-a814-9a14f8b3428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "topic_embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "perspective_model = SentenceTransformer(\"nli-roberta-base-v2\")\n",
    "# comment_perspectives = pipeline(task=\"text2text-generation\", model=\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804f03d-06e4-45cf-a013-422bb4422e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data = [ details[\"comment\"][\"plaintext\"] for details in details_collection.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729aa4bf-d1cb-42d2-a8e8-71fcc43c04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"topic_embeddings.npy\" in files:\n",
    "    topic_embeddings = np.load(\"topic_embeddings.npy\")\n",
    "else:\n",
    "    topic_embeddings = topic_embedder.encode(comment_data,show_progress_bar=True)\n",
    "    np.save(\"topic_embeddings.npy\", topic_embeddings)\n",
    "\n",
    "if \"perspective_embeddings.npy\" in files:\n",
    "    perspective_embeddings = np.load(\"perspective_embeddings.npy\")\n",
    "else:\n",
    "    perspective_embeddings = perspective_model.encode(comment_data,show_progress_bar=True)\n",
    "    np.save(\"perspective_embeddings.npy\", perspective_embeddings)\n",
    "\n",
    "# Concatenate topic + perspective embeddings\n",
    "final_embeddings = np.hstack((topic_embeddings, perspective_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e1c20-9b88-4faf-acbc-e7b863c11d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example timestamps (YYYY-MM-DD format)\n",
    "timestamps = pd.to_datetime(comments[\"Posted Date\"]).astype(int) / 10**9  # Convert to Unix timestamps\n",
    "timestamps = timestamps.to_numpy()\n",
    "\n",
    "# Normalize timestamps to [0,1] range\n",
    "scaler = MinMaxScaler()\n",
    "normalized_timestamps = scaler.fit_transform(timestamps.reshape(-1, 1))\n",
    "\n",
    "# Append normalized time to embeddings\n",
    "final_embeddings = np.hstack((final_embeddings, normalized_timestamps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de422377-8829-4d9b-8a07-84ce366bc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(topic_embeddings)\n",
    "\n",
    "# Apply HDBSCAN to cluster comments into topics\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, metric=\"euclidean\", cluster_selection_method=\"eom\")\n",
    "comments[\"topic_cluster\"] = clusterer.fit_predict(scaled_embeddings)\n",
    "\n",
    "# Count the number of topics\n",
    "num_topics = len(set(comments[\"topic_cluster\"])) - (1 if -1 in comments[\"topic_cluster\"].values else 0)\n",
    "print(f\"Number of Topics Identified: {num_topics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85f736-eaf3-4d4e-829c-f92653c2bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run PHATE\n",
    "phate_operator = phate.PHATE()\n",
    "phate_embedding = phate_operator.fit_transform(final_embeddings)\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(phate_embedding[:, 0], phate_embedding[:, 1], c=normalized_timestamps, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Time Progression\")\n",
    "plt.title(\"Comment Evolution Over Time\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMSE495-TwoSix",
   "language": "python",
   "name": "cmse495-twosix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
